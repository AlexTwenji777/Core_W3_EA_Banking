# -*- coding: utf-8 -*-
"""Core_Week_2_IP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HGD5JDGtAHuU7yd0AIQjEkRpWDM1t2rA

Financial Inclusion remains one of the main obstacles to economic and human development in Africa. For example, across Kenya, Rwanda, Tanzania, and Uganda only 9.1 million adults (or 13.9% of the adult population) have access to or use a commercial bank account.

The research problem is to figure out how we can predict which individuals are most likely to have or use a bank account. Your solution will help provide an indication of the state of financial inclusion in Kenya, Rwanda, Tanzania, and Uganda, while providing insights into some of the key demographic factors that might drive individuals’ financial outcomes.

In order to work on the above problem, you need to do the following:
1. Define the question, the metric for success, the context, experimental design taken and the appropriateness of the available data to answer the given question
2. Find and deal with outliers, anomalies, and missing data within the dataset.
3. Perform univariate, bivariate and multivariate analysis recording your observations.
4. Implement the solution by performing the respective analysis i.e. factor analysis, principal component analysis, and discriminant analysis.
5. Challenge your solution by providing insights on how you can make improvements.

# DATA PREPARATION

## LOADING DATASETS
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
from matplotlib import pyplot as plt
# %matplotlib inline

Variable_Definitions = pd.read_csv('VariableDefinitions.csv')
Variable_Definitions

Financial_Data = pd.read_csv('Financial Dataset - 1.csv')
Financial_Data.head()

Financial_Data.tail()

Financial_Data.info()

Financial_Data.describe()

Financial_Data.describe(include='all')

# Reanaming Columns to match the Variable Definitions Dataset
Financial_Data.columns = ['country', 'year', 'uniqueid', 'has_bank_account', 'location_type', 
                          'cellphone_access', 'household_size', 'age_of_respondent', 'gender_of_respondent', 
                          'relationship_with_head', 'marital_status', 'education_level', 'job_type']
Financial_Data.head()

# Checking for duplicated data
Financial_Data.duplicated().sum()

# Checking for Null values
Financial_Data.isnull().sum()

# Previewing Null values so as to know what to do with them.

Nulls = Financial_Data[Financial_Data.isnull().any(axis=1)]
Nulls

"""## DEFINING THE QUESTION

### a) Specifying the Question

Predict which individuals are most likely to have or use a bank account, while providing insights into some of the key demographic factors that might drive individuals’ financial outcomes.

### b) Defining the Metric for success
 
Finding the most deterministic factors for financial inclusion.

### c) Understanding the Context

Traditionally, access to bank accounts has been regarded as an indicator of financial inclusion. Despite the proliferation of mobile money in Africa and the growth of innovative fintech solutions, banks still play a pivotal role in facilitating access to financial services. Access to bank accounts enables households to save and facilitate payments while also helping businesses build up their credit-worthiness and improve their access to other financial services. Therefore, access to bank accounts is an essential contributor to long-term economic growth.

The goal is to determine which factors most contribute to owning bank accounts, hence contribute to financial inclusion.

### d) Recording the Experimental Design

1. Loading the Datasets (done Above)
2. Data Preparation (done Above)
3. Investigating the Dataset (done Above)
4. Data Cleaning
5. Exploratory Data Analysis (Univariate, Bivariate and Multivariate)
6. Answering the Question
7. Conclusions
8. Recommendations

### e) Data Relevance

This will be discussed from d) 6 to d) 8, after the analysis is complete

# DATA CLEANING
"""

# Determining the no. of records in our dataset

print('The number of rows is ' + str(Financial_Data.shape[0]) + ' and columns are ' +str(Financial_Data.shape[1]))

print('The records with Null Values has ' + str(Nulls.shape[0]) + ' rows and ' +str(Financial_Data.shape[1]) + ' columns')

Nulls_Percentage = (181 / 23524)*100
Nulls_Percentage

# The number of records with Nulls is less than 0.8 % of the data and can therefore be neglected during analysis.

# Dropping Nulls

Fin_No_Null = Financial_Data.dropna(axis=0)
Fin_No_Null.shape

"""# Exploratory Data Analysis (EDA)

## UNIVARIATE

## a) Numerical
"""

Fin_No_Null.info()

col_names = ['household_size','age_of_respondent', 'year']

fig, ax = plt.subplots(len(col_names), figsize= (8,40))

for i, col_val in enumerate(col_names):
  sns.boxplot(y = Fin_No_Null[col_val], ax= ax[i])
  ax[i].set_title('Box plot - {}'.format(col_val), fontsize= 10)
  ax[i].set_xlabel(col_val, fontsize= 8)
plt.show()

# from the boxplot below we can see there are a couple of outliers

# Preview of Year outliers
Fin_No_Null[Fin_No_Null.year > 2018]

# Since the data was taken in 2018, there can't be data from years after that, unless they are predictions.
# We will remove the 3 rows shown above

Fin_No_Null = Fin_No_Null[Fin_No_Null.year < 2019]
Fin_No_Null[Fin_No_Null.year > 2018]

# Checking for Anomalies

Q1_household_size = Fin_No_Null['household_size'].quantile(.25)
Q3_household_size = Fin_No_Null['household_size'].quantile(.75)

IQR_household_size = Q3_household_size - Q1_household_size

print('IQR households ' + str(IQR_household_size))

Q1_age_of_respondent = Fin_No_Null['age_of_respondent'].quantile(.25)
Q3_age_of_respondent = Fin_No_Null['age_of_respondent'].quantile(.75)

IQR_age_of_respondent = Q3_age_of_respondent - Q1_age_of_respondent

print('IQR age ' + str(IQR_age_of_respondent))

lower_bound_household_size = Q1_household_size - (1.5*IQR_household_size)
upper_bound_household_size = Q3_household_size + (1.5 * IQR_household_size)

print('lower_bound_household_size ' + str(lower_bound_household_size))
print('upper_bound_household_size ' + str(upper_bound_household_size))

lower_bound_age_of_respondent = Q1_age_of_respondent - (1.5*IQR_age_of_respondent)
upper_bound_age_of_respondent = Q3_age_of_respondent + (1.5 * IQR_age_of_respondent)

print('lower_bound_age_of_respondent ' + str(lower_bound_age_of_respondent))
print('upper_bound_age_of_respondent ' + str(upper_bound_age_of_respondent))

# Dealing with outliers and anomalies

Fin_No_Null = Fin_No_Null[Fin_No_Null.household_size <= upper_bound_household_size]
Fin_No_Null

Fin_No_Null = Fin_No_Null[Fin_No_Null.age_of_respondent < upper_bound_age_of_respondent]
Fin_No_Null

# In both cases, the lower bounds from the IQR were ignored, since they were negatives,
# which have no implications in household or age columns.

# Checking the new boxplots

col_names = ['household_size','age_of_respondent', 'year']

fig, ax = plt.subplots(len(col_names), figsize= (8,40))

for i, col_val in enumerate(col_names):
  sns.boxplot(y = Fin_No_Null[col_val], ax= ax[i])
  ax[i].set_title('Box plot - {}'.format(col_val), fontsize= 10)
  ax[i].set_xlabel(col_val, fontsize= 8)
plt.show()

# The anomalies in the household size and year columns were dealt with.
# The outliers in the age column could only be minimised.

"""## b) Categorical"""

Fin_No_Null.info()

# Records from different countries

Fin_No_Null.country.value_counts().plot.bar()
plt.title('East African Country Records')

Fin_No_Null.has_bank_account.value_counts().plot(kind= 'pie')
plt.title('Bank Account Holders')

Fin_No_Null.location_type.value_counts().plot(kind= 'bar')
plt.title('Location')

# More people live in the rural areas than the urban areas

Fin_No_Null.cellphone_access.value_counts().plot(kind= 'barh')
plt.title('Cellphone Access')

# There's a large access to cellphone as was mentioned before.

Fin_No_Null.gender_of_respondent.value_counts().plot(kind= 'barh')
plt.title('Gender')

#There are more females than males.

Fin_No_Null.relationship_with_head.value_counts().plot(kind= 'barh')
plt.title('Household Relationships')

# Most of the respondents were the heads of households, followed by their spouse.

Fin_No_Null.marital_status.value_counts().plot(kind= 'barh')
plt.title('Marital Status')

# Most of the respondents were married, followed by singles.

Fin_No_Null.education_level.value_counts().plot(kind= 'barh')
plt.title('Education Level')

# Most EA respondents only have a Primary education. '6' needs to be 
# investigated further.

Fin_No_Null.job_type.value_counts().plot(kind= 'barh')
plt.title('Jobs')

# Most participants are Self employed, Informally employed or Farmers and Fisher(men/women)

"""## c) Summary Statistics"""

Fin_No_Null.describe()

Fin_No_Null.describe(include= 'all')

# The Central Tendencies, Quantiles and Standard deviations are shown below

print('The mode of year: '+str(Fin_No_Null.year.mode()))
print('The mode of household size: '+str(Fin_No_Null.household_size.mode()))
print('The mode of age: '+str(Fin_No_Null.age_of_respondent.mode()))

# The modes of the numeric fields are unimodal, indicating the data is sampled from one population i.e. East Africa.

# Ranges of numeric data

print('The range of year: ' +str(Fin_No_Null.year.max()- Fin_No_Null.year.min()))
print('The range of household size: ' +str(Fin_No_Null.household_size.max() - Fin_No_Null.household_size.min()))
print('The range of ages: ' +str(Fin_No_Null.age_of_respondent.max() - Fin_No_Null.age_of_respondent.min()))

# The range shows that the dataset has 3 years worth of data, 10 different numeric values for the household sizes and
# 68 different ages.

# Standard Deviation of Numeric Data

print('The standard deviation of year: '+str(Fin_No_Null.year.std()))
print('The standard deviation of household size: '+str(Fin_No_Null.household_size.std()))
print('The standard deviation of age: '+str(Fin_No_Null.age_of_respondent.std()))

# The standard deviations mirror the ranges, in that the values with more range show higher
# values of standard deviation, meaning the data is more spread out with increasing deviation.

# Variance of Numeric Data

print('The variance of year: '+str(Fin_No_Null.year.var()))
print('The variance of household size: '+str(Fin_No_Null.household_size.var()))
print('The variance of age: '+str(Fin_No_Null.age_of_respondent.var()))

# Similar to standard deviation above in that, The greater the variance, the greater the spread 
# in the data about the mean.

# Quantiles of Numeric Data

print('The quantiles of year: \n'+str(Fin_No_Null.year.quantile([0.25,0.5,0.75])))
print('The quantiles of household size: \n'+str(Fin_No_Null.household_size.quantile([0.25,0.5,0.75])))
print('The quantiles of age: \n'+str(Fin_No_Null.age_of_respondent.quantile([0.25,0.5,0.75])))

# The quantiles indicate Q1(0.25) - point where 25% of the data is below or equal to (2016 for year, 2 for household size and 26 for age)
# Q2(0.5) - point where 50% of the data is below or equal to (2017 for year, 3 for household size and 35 for age). It is also the median.
# Q3(0.75) - point where 75% of the data is below or equal to (2018 for year, 5 for household size and 48 for age).

# Skewness of Numeric Data

print('The skewness of year: '+str(Fin_No_Null.year.skew()))
print('The skewness of household size: '+str(Fin_No_Null.household_size.skew()))
print('The skewness of age: '+str(Fin_No_Null.age_of_respondent.skew()))

# The positive values indicates that the tail of the data is right-skewed.
# The skew of year seems almost negligibe and could be symmetrical/non-skewed

# Kurtosis of Numeric Data

print('The kurtosis of year: '+str(Fin_No_Null.year.kurt()))
print('The kurtosis of household size: '+str(Fin_No_Null.household_size.kurt()))
print('The kurtosis of age: '+str(Fin_No_Null.age_of_respondent.kurt()))

# The data has negative kurtosis indicating that the distribution has lighter tails 
# and a flatter peak than the normal distribution.

"""## d) Univariate Analysis Recommendation

The categorical columns e.g. relationship_with_head, marital_status, education_level and job_type can be converted to numbers by numeric encoding in order to further analyse them.

## BIVARIATE

## a) Numeric
"""

Fin_No_Null.info()

fig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=(10, 7))
fig.suptitle('Numerical Relationships')
sns.scatterplot(x= Fin_No_Null.year, y= Fin_No_Null.household_size, ax=ax1)
sns.scatterplot(x= Fin_No_Null.year, y= Fin_No_Null.age_of_respondent, ax=ax2)
sns.scatterplot(x=Fin_No_Null.age_of_respondent , y= Fin_No_Null.household_size, ax=ax3)
plt.show()

# There seems to be no correlation among any of the numeric columns

pearson_coeff = Fin_No_Null["household_size"].corr(Fin_No_Null["age_of_respondent"], method="pearson") 
pearson_coeff

# As suspected from the scatterplot above, the pearson correlation shows very weak negative correlation,
# Virtually, there's no correlation between household size and age of respondent.

sns.pairplot(Fin_No_Null)
plt.show()
# This further confirms the lack of correlation

#The heatmap cements our lack of correlation analysis

sns.heatmap(Fin_No_Null.corr(),annot=True)
plt.show()

"""## b) Categorical"""

Fin_No_Null.info()

fig, (ax1,ax2) = plt.subplots(1,2, figsize=(14, 7))
fig.suptitle('Categorical-Numerical Relationships')
sns.barplot(x= Fin_No_Null.country, y= Fin_No_Null.household_size, ax=ax1)
sns.barplot(x= Fin_No_Null.country, y= Fin_No_Null.age_of_respondent, ax=ax2)
plt.show()

# Uganda and Rwanda had the highest household sizes, each ~4.5 people per house, while Tanzania had the least at ~2. 
# In terms of age, all countries had similar averages between 35 - 39 years.

fig, (ax1,ax2) = plt.subplots(1,2, figsize=(14, 7))
fig.suptitle('Categorical-Numerical Relationships')
sns.barplot(x= Fin_No_Null.location_type, y= Fin_No_Null.household_size, ax=ax1)
sns.barplot(x= Fin_No_Null.location_type, y= Fin_No_Null.age_of_respondent, ax=ax2)
plt.show()

# Rural areas had an average of 4 people per house while urban areas had ~2.8. 
# The ages were also almost similar between 37 - 39 years.

fig ,ax = plt.subplots(1, figsize=(10, 7))
fig.suptitle('Categorical-Numerical Relationships')
sns.barplot(x= Fin_No_Null.cellphone_access, y= Fin_No_Null.age_of_respondent, ax=ax)
plt.show()

# The average age of respondents with cellphone access was 37.5 years, while those that did not have was 40 years.

fig, (ax1,ax2) = plt.subplots(1,2, figsize=(14, 7))
fig.suptitle('Categorical-Numerical Relationships')
sns.barplot(x= Fin_No_Null.gender_of_respondent, y= Fin_No_Null.household_size, ax=ax1)
sns.barplot(x= Fin_No_Null.gender_of_respondent, y= Fin_No_Null.age_of_respondent, ax=ax2)
plt.show()

# Neither age nor household size was significantly different when compared to the gender.

fig, (ax1,ax2) = plt.subplots(2,1, figsize=(10, 10))
fig.suptitle('Categorical-Numerical Relationships')
sns.barplot(x= Fin_No_Null.relationship_with_head, y= Fin_No_Null.household_size, ax=ax1)
sns.barplot(x= Fin_No_Null.relationship_with_head, y= Fin_No_Null.age_of_respondent, ax=ax2)
plt.show()

# Households with average of 3 members had most respondents being the Head of the household 
# whereas those with an average over 4 had children as the most respondents.
# Respondents with an average of over 40 years tended to be the heads of the house.

fig, (ax1,ax2) = plt.subplots(1,2, figsize=(20, 7))
fig.suptitle('Categorical-Numerical Relationships')
sns.barplot(x= Fin_No_Null.marital_status, y= Fin_No_Null.household_size, ax=ax1)
sns.barplot(x= Fin_No_Null.marital_status, y= Fin_No_Null.age_of_respondent, ax=ax2)
plt.show()

# Households with an average of over 4 people were largely dominated by married/living together couples.
# Households with average age approaching 60 were dominated by widowed people.

fig, (ax1,ax2) = plt.subplots(2,1, figsize=(18, 14))
fig.suptitle('Categorical-Numerical Relationships')
sns.barplot(x= Fin_No_Null.education_level, y= Fin_No_Null.household_size, ax=ax1)
sns.barplot(x= Fin_No_Null.education_level, y= Fin_No_Null.age_of_respondent, ax=ax2)
plt.show()

# Households with 4 people were dominated by Secondary education recipients whereas people approaching 50 years largely have no formal education.

fig, (ax1,ax2) = plt.subplots(2,1, figsize=(30, 14))
fig.suptitle('Categorical-Numerical Relationships')
sns.barplot(x= Fin_No_Null.job_type, y= Fin_No_Null.household_size, ax=ax1)
sns.barplot(x= Fin_No_Null.job_type, y= Fin_No_Null.age_of_respondent, ax=ax2)
plt.show()

# Bigger households largely depended on farming and fishing.
# The elderly (60yrs) largely depended on the Government for their livelihood.

"""## c) Bivariate Analysis Recommendations

The data '6' in educational level does not have a proper reference, hence we can only assume that it means the level of education referred to is standard/grade 6. Another option is to ignore it, however as can be seen, there's a substatial amount of data that holds this value. Further research needs to be done to determine what this value actually represents.

Furthermore T-test cannot be conducted since the variance between the 2 numerical values isn’t homogenous.

## MULTIVARIATE

### a) PCA (Principal Component Analysis)
"""

Fin_No_Null.info()

# Data for label encoding

Financial_encoding = Fin_No_Null.copy(deep=True)
Financial_encoding['education_level'] = Financial_encoding['education_level'].astype('category')
Financial_encoding['job_type'] = Financial_encoding['job_type'].astype('category')
Financial_encoding['country'] = Financial_encoding['country'].astype('category')
Financial_encoding['location_type'] = Financial_encoding['location_type'].astype('category')
Financial_encoding['cellphone_access'] = Financial_encoding['cellphone_access'].astype('category')
Financial_encoding['gender_of_respondent'] = Financial_encoding['gender_of_respondent'].astype('category')
Financial_encoding['relationship_with_head'] = Financial_encoding['relationship_with_head'].astype('category')
Financial_encoding['marital_status'] = Financial_encoding['marital_status'].astype('category')
Financial_encoding.info()

# Label encoding the categorical data

from sklearn.preprocessing import LabelEncoder

labelencoder = LabelEncoder()

Financial_encoding['education_level'] = labelencoder.fit_transform(Financial_encoding['education_level'])
Financial_encoding['job_type'] = labelencoder.fit_transform(Financial_encoding['job_type'])
Financial_encoding['country'] = labelencoder.fit_transform(Financial_encoding['country'])
Financial_encoding['location_type'] = labelencoder.fit_transform(Financial_encoding['location_type'])
Financial_encoding['cellphone_access'] = labelencoder.fit_transform(Financial_encoding['cellphone_access'])
Financial_encoding['gender_of_respondent'] = labelencoder.fit_transform(Financial_encoding['gender_of_respondent'])
Financial_encoding['relationship_with_head'] = labelencoder.fit_transform(Financial_encoding['relationship_with_head'])
Financial_encoding['marital_status'] = labelencoder.fit_transform(Financial_encoding['marital_status'])
Financial_encoding.head()

x = Financial_encoding.drop(['has_bank_account', 'uniqueid'], axis= 1) # features
y = Financial_encoding['has_bank_account'] #target variable

# Splitting the data into train and test sets

from sklearn.model_selection import  train_test_split

x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=0)

x_train.head()

y_train.head()

# train sets have similar index

x_test.head()

y_test.head()

# test sets have similar index

# Performing standard scalar normalization to normalize our feature set.

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
x_train =sc.fit_transform(x_train)
x_test = sc.transform (x_test)

# Applying PCA
# We did not specify the number of components in the constructor. 
# Hence, the features in Financial_encoding set will be returned for both the training and test sets.

from sklearn.decomposition import PCA

pca = PCA()
X_train = pca.fit_transform(x_train)
X_test = pca.transform(x_test)

# Explained Variance Ratio

explained_variance = pca.explained_variance_ratio_
explained_variance

# From below, we can see that the first 10 (neglecting the last principal component 
# gives us an accuracy of 96.19%)

"""### Improving the PCA model"""

# Using 1 Principal Component

pca = PCA(n_components=1)
X_train = pca.fit_transform(x_train)
X_test = pca.transform(x_test)

from sklearn.ensemble import RandomForestClassifier

classifier = RandomForestClassifier(max_depth=2, random_state=0)
classifier.fit(X_train, y_train)

# Predicting the Test set results
y_pred = classifier.predict(X_test)

# Performance Evaluation

from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score

cm = confusion_matrix(y_test, y_pred)
print(cm)
print('Accuracy' , accuracy_score(y_test, y_pred))

# The Accuracy has dropped to 85.75 % when using one principal component

# Using 2 Principal Components

pca = PCA(n_components=2)
X_train = pca.fit_transform(x_train)
X_test = pca.transform(x_test)

classifier = RandomForestClassifier(max_depth=2, random_state=0)
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)

cm = confusion_matrix(y_test, y_pred)
print(cm)
print('Accuracy' , accuracy_score(y_test, y_pred))

# There's no change in accuracy, therefore all features seem to be contributing to the results.

"""### b) LDA (Linear Discriminate Analysis)"""

x = Financial_encoding.drop(['has_bank_account', 'uniqueid'], axis= 1) # features
y = Financial_encoding['has_bank_account'] #target variable

# Split the data

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)

# Feature Scaling

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

# Peforming LDA
# We have to pass the value for the n_components parameter of the LDA, 
# which refers to the number of linear discriminates that we want to retrieve.

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
lda = LDA(n_components=1)
X_train = lda.fit_transform(x_train, y_train)
X_test = lda.transform(x_test)

# Training and Making Predictions

from sklearn.ensemble import RandomForestClassifier

classifier = RandomForestClassifier(max_depth=2, random_state=0)
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)

# Evaluating the Performance

from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score

cm = confusion_matrix(y_test, y_pred)
print(cm)
print('Accuracy' + str(accuracy_score(y_test, y_pred)))

# The accuracy with 1 LDA is higher than that with 1 PCA which had 85% accuracy.

"""# IMPLEMENTING THE SOLUTION"""

Bank_Account = Fin_No_Null[Fin_No_Null.has_bank_account == 'Yes']
Bank_Account.head()

Bank_Account.country.value_counts()

# Kenyans have the most accounts

Bank_Account.location_type.value_counts()

# Location doesn't matter

Bank_Account.cellphone_access.value_counts()

# Most have a cell-phone

Bank_Account.household_size.value_counts()

# Generally, households with less people have more banking access

Bank_Account.age_of_respondent.value_counts()

# Generally, people between 28 - 40 years old have more banking access

Bank_Account.gender_of_respondent.value_counts()

# More males than feales have bank accounts

Bank_Account.relationship_with_head.value_counts()

# The heads of households have more acces to accounts

Bank_Account.marital_status.value_counts()

# Married/Living together people have more bank accounts

Bank_Account.education_level.value_counts()

# People with some form of education have more acces to bank accounts

Bank_Account.job_type.value_counts()

# Type of job affects the banking access

Bank_Account.year.value_counts()

# As expected, as the years progress, the access to banking improves.

"""# CONCLUSION

Location doesn’t matter (rural or urban) and could be the feature that we neglect in the prediction model. All the other factors affect the access to banking.

# RECOMMENDATION

For better analysis, more numerical values should be included in the data collection phase. Ideally, the categorical responses should be serialised in numbers so as to enable efficient analysis that will enable correlations to be realised better.
"""

